{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ST06HtdPDTJv"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "R1GU6TZCDnQ6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(\"I love Bangladesh\",\n",
        "                   return_tensors='pt')\n",
        "#return_tensors='pt', the tokenizer instead returns the output as PyTorch tensors\n",
        "#\"pt\" = PyTorch,\"tf\" = TensorFlow, and \"np\" = NumPy.\n",
        "\n",
        "#this code is returning us token ids, token tyoe ids, and attention mask"
      ],
      "metadata": {
        "id": "9ugyrSXeDr72"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens #inn tensor 101 is CLS, 102 is separator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLqUxAADDvuz",
        "outputId": "69583364-f803-4d03-9be5-26046b93cf5d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 1045, 2293, 7269,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer([\"I love Bangladesh\", \"But hardly this country loves us\"],\n",
        "                   padding=True,\n",
        "                   return_tensors='pt')\n",
        "tokens\n",
        "#padding will make the tensor length of same size and add 0s to match the length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge3pSFYbEzm8",
        "outputId": "06836623-2131-4eed-b706-e4aad8cb50c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 1045, 2293, 7269,  102,    0,    0,    0],\n",
              "        [ 101, 2021, 6684, 2023, 2406, 7459, 2149,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer([\"I love Bangladesh\", \"But hardly this country loves us\"],\n",
        "                   padding=\"max_length\",\n",
        "                   max_length=15,\n",
        "                   return_tensors='pt')\n",
        "tokens\n",
        "#adding max length in padding to have more control ovper the padding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjPVFB4iFifT",
        "outputId": "d12fef15-31b6-4ee6-b4ef-958084afde97"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 1045, 2293, 7269,  102,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0],\n",
              "        [ 101, 2021, 6684, 2023, 2406, 7459, 2149,  102,    0,    0,    0,    0,\n",
              "            0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer([\"I love Bangladesh\", \"But hardly this country loves us\"],\n",
        "                   padding=\"max_length\",\n",
        "                   max_length=5,\n",
        "                   truncation=True,\n",
        "                   return_tensors='pt')\n",
        "tokens\n",
        "\n",
        "#truncation wiill help us to map the data accordiing to our max_length tokens\n",
        "#if the token size is bigger than max_length, truncation will map than token without an error\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYjHZzapGTXI",
        "outputId": "21a29cde-52c2-4a87-bcf3-05a03b8a86f3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 1045, 2293, 7269,  102],\n",
              "        [ 101, 2021, 6684, 2023,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(\"I love Bangladesh\",\n",
        "                   return_tensors='pt')"
      ],
      "metadata": {
        "id": "bSKoDlBiIwO-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "output = model(**tokens)\n",
        "\n",
        "#creating contexual embeddings"
      ],
      "metadata": {
        "id": "kXi_CZKdIx_P"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output['last_hidden_state']\n",
        "#for each token of \"CLS I love Bangladesh SEP\" last_hidden_state is shoowing the contexual embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI6JppsYJEmu",
        "outputId": "d8028a92-a9f4-4950-e851-56b89d752597"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0224,  0.2553, -0.2280,  ..., -0.2571,  0.1985,  0.3660],\n",
              "         [ 0.2632,  0.5703,  0.0819,  ..., -0.4229,  0.5411, -0.5982],\n",
              "         [ 1.0946,  0.9915,  0.5911,  ..., -0.3156,  0.3700, -0.2465],\n",
              "         [ 0.1894, -0.2037, -0.4778,  ...,  0.4723,  0.0961, -0.5052],\n",
              "         [ 0.7222,  0.2883, -0.2963,  ..., -0.2011, -0.4999, -0.4110]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output['last_hidden_state'].shape\n",
        "#berts embedding shape is 768"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS1GMek6JIFZ",
        "outputId": "194eec3a-eb01-47ce-e0ad-c1130b020a61"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EnRjkxbAJfuk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}